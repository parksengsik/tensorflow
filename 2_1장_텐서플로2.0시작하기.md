## 2_1. 텐서플로 2.0 시작하기
### 2.1 텐서플로 기초 
#### 2.1.1 난수 생성
* 랜덤은 신경망에서 꼭 필요한 기능입니다.
* 신경망을 쉽게 정의해보면 많은 숫자로 구성된 행렬이라고 할 수 있습니다, 이 행렬에 어떤 입력을 넣으면 출력을 얻게 되고, 잘 작동할 경우 원하는 출력에 점점 가까워지게 됩니다.
* 행렬을 구성하는 숫자를 구하기위해서는 처음에는 숫자들을 랜덤한 값으로 지정해 줄수 밖에 없습니다.
* 신경망의 초깃값을 지정해주는 것을 초기화라고 하며, 현재 가장 많이 쓰이는 방법은 Xavier 초기화와 He 초기화인데, 이 방법들은 랜덤하지만 어느 정도 규칙성이 있는 범위내에서 난수를 지정합니다.
```python
# 균일 분포의 난수
rand = tf.random.uniform([1],0,1)
print(rand)
>> tf.Tensor([0.21856475], shape=(1,), dtype=float32)
```
* tf.random.unoiform 함수를 불러오면 균일 분포의 난수를 얻을 수 있습니다.
    + 균일 분포란 최솟값과 최댓값 사이의 모든 수가 나올 확률이 동일한 분포에서 수를 뽑는다는 뜻입니다.
```python
# 정규 분포의 난수
rand = tf.random.normal([4],0,1)
print(rand)
>> tf.Tensor([-0.70884943 -0.15189779  1.3982424  -0.76306903], shape=(4,), dtype=float32)
```
* tf.random.normal 함수를 불러오면 정규 분포의 난수를 얻을 수 있습니다.
    + 정규 분포란 가운데가 높고 양극단으로 갈수록 낮아져서 종 모양을 그리는 분포입니다.
    + 여기서는 1이상의 값도 나오고 음수의 값도 나오는 데, 여기서는 두 번째의 0은 정규 분포의 평균, 세 번째의 1은 정규 분포의 표준편자를 의미합니다.
* Xavier 초기화나 He 초기화는 균일 분포와 정규 분포 중 하나를 택해서 신경망을 초기화합니다.

#### 2.1.2 뉴런 만들기 
* 이전에는 뉴런을 퍼셉트론이라고도 불렀으며, 입력을 받아서 계산 후 출력을 반환하는 단순한 구조입니다.
* 신경망은 뉴런이 여러 개 모여 레이어를 구성한 후, 이 레이어가 다시 모여 구성돤 형태입니다.
* 뉴런은 입력, 가중치, 활성화함수, 출력으로 구성됩니다.
    + 입력, 가중치, 출력은 보통 정수나 float가 많이 쓰입니다.
    + 활성화함수는 뉴런의 출력값을 정하는 함수입니다.
    + 뉴런에서 학습할 때 변하는 것은 가중치입니다.
    + 가중치는 처음에는 초기화를 통해 랜덤한 값을 넣고, 학습과정에서 점차 일정한 값으로 수렴합니다.
    + 활성화함수로는 시그모이드, ReLU 등을 주로 쓰게 됩니다.
        - 시그모이드는 S자 형태의 곡선이라는 뜻입니다.
        - ReLU는 정류된 선형 함수라는 뜻입니다, 딥러닝에서 선형 함수는 y=x라는 식으로 정의할 수 있는 입력과 출력이 동일한 함수를 의미합니다. 이 함수를 사용하면 음수 값을 0으로 만듭니다.
```python
# 시그모이드 함수
import math
def sigmoid(x):
    return 1 / (1 + math.exp(-x))
```

```python
# 뉴런의 입력과 출력 정의
x = 1
y = 0
w = tf.random.normal([1],0,1)
output = sigmoid(x * w)
print(output)
>> 0.38418757481926114
```
* 입력인 x에는 1을 넣고 가중치는 w로 정규 분포의 랜덤한 값을 넣습니다, 실제출력인 output은 sigmoid()함수에 입력과 출력을 곱한 값을 넣어서 계산합니다.
* 실제출력으로 나온 0.3841과 기대출력인 0의 차이는 0 - 0.3841 = -0.3841을 에러라고 합니다, 뉴런의 학습은 이 에러가 0에 가까워지게 해서 출력으로 기댓값에 가까운 값을 얻는 것입니다.
```python
# 경사 하강법을 이용한 뉴런의 학습
for i in range(0,1000):
    output = sigmoid(x * w)
    error = y - output
    w = w + x * 0.1 * error

    if i % 100 == 99:
        print(i, error, output)
>> 99 -0.14884271100433677 0.14884271100433677
199 -0.06325179337017506 0.06325179337017506
299 -0.039468389772562444 0.039468389772562444
399 -0.028556695817354417 0.028556695817354417
499 -0.02233235363196654 0.02233235363196654
599 -0.018319760751561102 0.018319760751561102
699 -0.015521780632501191 0.015521780632501191
799 -0.013461099042272094 0.013461099042272094
899 -0.011881038900144624 0.011881038900144624
999 -0.01063141478605975 0.01063141478605975
```
* 위의 코드는 경사 하강법은 w에 입력과 학습률(a)과 에러를 곱한 값을 더해주는 것입니다.
* 학습률은 w를 업데이트하는 정도로, 큰 값으로 설정하면 학습이 빨리 되지만 과도한 학습으로 적정한 수치를 벗어날 우려가 있고, 너무 작은 값으로 설정하면 핫급속도가 너무 느려질 수 있습니다. 여기서는 a =0.1로 설정하겠습니다.
```python
# x = 0일 때 y = 1을 얻는 뉴런의 학습
x = 0
y = 1
w = tf.random.normal([1],0,1)

for i in range(0,1000):
    output = sigmoid(x * w)
    error = y - output
    w = w + x * 0.1 * error

    if i % 100 == 99:
        print(i, error, output)
>> 99 0.5 0.5
199 0.5 0.5
299 0.5 0.5
399 0.5 0.5
499 0.5 0.5
599 0.5 0.5
699 0.5 0.5
799 0.5 0.5
899 0.5 0.5
999 0.5 0.5
```
* 위의 코드는 x=0이기 때문에 w에 더해지는 값은 없습니다. 결국 1000번의 실행 동안 w 값은 변하지않습니다.
* 이런 경우를 방지하기 위해 편향이라는 것을 뉴런에 넣어줍니다.
* 편향이라는 말처럼 입력으로는 늘 한쪽으로 치우친 고정된 값을 받아서 입력으로 0을 받았을 때 뉴런이 아무것도 배우지 못하는 상황을 방지합니다.
* 편향은 w처럼 난수로 초기화되며 뉴런에 더해져서 출력을 계산하게 됩니다.
* 수식에서는 관용적으로 bias의 앞글자인 b를 씁니다. 
```python
# x = 0일때 y = 1을 얻는 뉴런의 학습에 편향을 더함
x = 0
y = 1
w = tf.random.normal([1],0,1)
b = tf.random.normal([1],0,1)

for i in range(0,1000) : 
    output = sigmoid(x*w+1*b)
    error = y - output
    w = w + x * 0.1 * error
    b = b + 1 * 0.1 * error

    if i % 100 == 99 :
        print(i,error,output)
>> 99 0.10177939597023222 0.8982206040297678
199 0.05225158597680568 0.9477484140231943
299 0.034802091617417985 0.965197908382582
399 0.026009043696925693 0.9739909563030743
499 0.020735070990611115 0.9792649290093889
599 0.017227181573013728 0.9827728184269863
699 0.014728337063920405 0.9852716629360796
799 0.01285917701759598 0.987140822982404
899 0.01140895023999211 0.9885910497600079
999 0.010251397988400357 0.9897486020115996
```
