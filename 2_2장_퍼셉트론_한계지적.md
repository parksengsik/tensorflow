## 2_2. 퍼셉트론의 한계지적
### 소개
* 뉴런의 원조격인 퍼셉트론의 한계 지적에서 쓰인 AND, OR, XOR 연산을 할 수 있는 신경망 네트워크를 만들어봅시다.
### 퍼셉트론의 한계 지적에서 쓰인 AND,OR,XOR 연산 신경망 네트워크 
#### 1. 라이브러리와 함수 선언하기. 
```python
import tensorflow as tf
import numpy as np
import math

# 시그모이드 함수
def sigmoid(x):
    return 1 / (1 + math.exp(-x))
```

#### 2. 첫 번째 신경망 네트워크 AND를 만들어보자. 
```python
# 첫 번째 신경망 네트워크 : AND
x = np.array([[1,1],[1,0],[0,1],[0,0]])
y = np.array([[1], [0], [0], [0]])
w = tf.random.normal([2],0,1)
b = tf.random.normal([1],0,1)
b_x = 1

for i in range(0,2000) :
    error_sum = 0
    for j in range(4) :
        output = sigmoid(np.sum(x[j]*w)+b_x*b)
        error = y[j][0] - output
        w = w + x[j] * 0.1 * error
        b = b + b_x * 0.1 * error
        error_sum += error
    if i % 200 == 99 :
        print(i, error_sum)
        
# 출력
>> 99 -0.16087775134773874
299 -0.08033600003478689
499 -0.05371569062794952
699 -0.040213792818322526
899 -0.03206287113584773
1099 -0.026622329787033702
1299 -0.022738829488517135
1499 -0.019832152784422918
1699 -0.01757735643714221
1899 -0.015777930891931594
```
```python
# AND 네트워크 평가
for i in range(4):
    print('X:', x[i], 'Y:', y[i], 'Output:', sigmoid(np.sum(x[i]*w)+b))
    
# 출력
>> X: [1 1] Y: [1] Output: 0.964966100605754
X: [1 0] Y: [0] Output: 0.02482694744439901
X: [0 1] Y: [0] Output: 0.024902606770609598
X: [0 0] Y: [0] Output: 2.3605033365351133e-05
```
* 각 실제출력이 기대출력에 가까이 나오는 것을 확인할 수 있습니다.

#### 3. 두 번째 신경망 네트워크 OR을 만들어보자. 
```python
# 두 번째 신경망 네트워크 : OR
x = np.array([[1,1],[1,0],[0,1],[0,0]])
y = np.array([[1],[1],[1],[0]])
w = tf.random.normal([2],0,1)
b = tf.random.normal([1],0,1)
b_x = 1

for i in range(2000):
    error_sum = 0
    for j in range(4):
        output = sigmoid(np.sum(x[j]*w)+b_x*b)
        error = y[j][0] - output
        w = w + x[j] * 0.1 * error
        b = b + b_x * 0.1 * error
        error_sum += error

    if i % 200 == 99:
        print(i,error_sum)
        
# 출력
>> 99 -0.09950230740597865
299 -0.03535990538173148
499 -0.021324927348043338
699 -0.015184427703754777
899 -0.011758584064853056
1099 -0.00958036498602921
1299 -0.008076285047359903
1499 -0.006976768156483848
1699 -0.006138217367316921
1899 -0.005478279915358444
```
```python
# OR 네트워크 평가
for i in range(4):
    print('X:', x[i], 'Y:', y[i], 'Output:', sigmoid(np.sum(x[i]*w)+b))
    
# 출력
>> X: [1 1] Y: [1] Output: 0.9999971256818193
X: [1 0] Y: [1] Output: 0.9896936211856563
X: [0 1] Y: [1] Output: 0.9896728222316397
X: [0 0] Y: [0] Output: 0.025769302574021546
```
* 각 실제출력이 기대출력에 가까이 나오는 것을 확인할 수 있습니다.

#### 4. 세 번째 신경망 네트워크 XOR을 만들어보자. 
```python
# 세 번째 신경망 네트워크 : XOR
x = np.array([[1,1], [1,0], [0,1], [0,0]])
y = np.array([[0], [1], [1], [0]])
w = tf.random.normal([2],0,1)
b = tf.random.normal([1],0,1)
b_x = 1

for i in range(2000) :
    error_sum = 0
    for j in range(4):
        output = sigmoid(np.sum(x[j]*w)+b_x*b)
        error = y[j][0] - output
        w = w + x[j] * 0.1 * error
        b = b + b_x *0.1 * error
        error_sum += error
    
    if i % 200 == 199:
        print(i,error_sum)
        
# 출력
>> 199 -0.00044523298959109336
399 -1.8105052288719392e-05
599 -7.464297520076713e-07
799 -8.376394911024931e-09
999 3.722842145670313e-09
1199 3.722842145670313e-09
1399 3.722842145670313e-09
1599 3.722842145670313e-09
1799 3.722842145670313e-09
1999 3.722842145670313e-09
```
```python
# XOR 네트워크 평가
for i in range(4):
    print('X:', x[i], 'Y:', y[i], 'Output:', sigmoid(np.sum(x[i]*w)+b))
    
# 출력
>> X: [1 1] Y: [0] Output: 0.5128176286712095
X: [1 0] Y: [1] Output: 0.5128176305326305
X: [0 1] Y: [1] Output: 0.4999999990686774
X: [0 0] Y: [0] Output: 0.5000000009313226
```
* 각 실제출력이 기대출력에 가까이 가지못한 것을 볼수있습니다, 위의 AND와 OR과는 다르게 실제출력 값이 0.5에 다 가까이 나오게 됩니다.
* 이것이 바로 첫번째 인공지능의 겨울이라는 불러온 것으로 잘 알려진 XOR 문제입니다.
* 하나의 퍼셉트론으로는 간단한 XOR 연상자도 만들어 낼 수 없다는 것을 증명했습니다.

#### 5. 신경망 네트워크 XOR을 tf.keras를 이용하여 만들어보자
* tf.keras에는 딥러닝 계산을 간편하게 하기 위한 추상적인 클래스인 model이 있습니다, 쉽게 말해서 딥러닝 계산을 위한 여러함수오 변수의 묶음이라고 생각하면 됩니다.
* model은 tf.keras에서 딥러닝을 계산하는 가장 핵심적인 단위입니다.
```python
# tf.keras를 이용한 XOR 네트워크 연산
x = np.array([[1,1], [1,0], [0,1], [0,0]])
y = np.array([[0],[1],[1],[0]])

model = tf.keras.Sequential([
                             tf.keras.layers.Dense(units=2, activation='sigmoid', input_shape=(2,)),
                             tf.keras.layers.Dense(units=1, activation='sigmoid')
])

model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.1), loss = 'mse')

model.summary()

# 출력
>> Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 2)                 6         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 3         
=================================================================
Total params: 9
Trainable params: 9
Non-trainable params: 0
_________________________________________________________________
```
* model에서 가장 많이 쓰이는 구조가 tf.keras.Sequential 구조입니다, 이것은 순차적으로 뉴런과 뉴런이 합쳐진 단위인 레이어를 일직선으로 배치한 것입니다.
* tf.keras.layers.Dense는 model에서 사용하는 레이어를 정의하는 명령입니다, Dense는 가장 기본적인 레이어로서, 레이어의 입력과 출력사이에 있는 모든 뉴런이 서로 연결되는 레이어입니다.
* tf.keras.layers.Dense 안의 units는 레이어를 구성하는 뉴런의 수를 정의합니다, 뉴런이 많을수록 일반적으로 레이어의 성능은 좋아지지만 계산량 또한 많아지고 메모리도 많이 차지하게 됩니다.
* tf.keras.layers.Dense 안의 activation은 우리가 계속 봐 온 활성화함수입니다.
* tf.keras.layers.Dense 안의 input_shape은 Sequential model의 첫 번째 레이어에서만 정의하는데, 입력의 차원 수가 어떻게 되는지를 정의합니다.
* 최적화 함수(optimizer)는 딥러닝의 학습식을 정의하는 부분입니다, 원래는 복잡한 수학을 써야 하지만, tf.keras에서는 이렇게 미리 정의된 최적화 함수를 불러오는것으로 바로 사용할 수 있습니다.
* SGD는 확률적 경사 하강법의 약자이며, 가중치를 업데이트할 때 미분을 통해 기울기를 구한 다음 기울기가 낮은 쪽으로 업데이트 하겠다는 뜻이고, 확률적은 전체를 한번에 계산하지 않고 확률적으로 일부 샘플을 구해서 조금씩으로 나눠서 계산하겠다는 뜻입니다.
* 손실(loss)은 앞에서 살펴본 error와 비슷한 개념입니다, 딥러닝은 보통 이 손실을 줄이는 방향으로 학습합니다.
* mse는 평균 제곱 오차의 약자로, 기대출력에서 실제출력을 뺀 뒤에 제곱한 값을 평균하는 것입니다.

```python
# tf.keras를 이용한 2-레이어 XOR 네트워크 학습
history = model.fit(x, y, epochs=2000, batch_size=1)

# 출력
>> ...
Epoch 1992/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0932
Epoch 1993/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0929
Epoch 1994/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0927
Epoch 1995/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0924
Epoch 1996/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0922
Epoch 1997/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0920
Epoch 1998/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0917
Epoch 1999/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0915
Epoch 2000/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.0913
```
* model.fit() 함수는 for문을 실해하는 것처럼 에포크(epochs)에 지정된 횟수만큼 학습시킵니다.
* batch_size는 한번에 학습시키는 데이터의 수인데, 여기서는 1로 지정해서 입력을 넣었을 때 정확한 값을 출력하는지 알아보려고 합니다.
```python
# tf.keras를 이용한 2-레이어 XOR 네트워크 평가
model.predict(x)

# 출력
>> array([[0.24899915],
       [0.74529886],
       [0.6281929 ],
       [0.3015707 ]], dtype=float32)
```
* model.predict() 함수에 입력을 넣으면 네트워크의 출력 결과를 알 수 있습니다.
* 결과를 봤을때 기대출력에 가까운 값이 출력 되는 것을 볼 수 있습니다.
